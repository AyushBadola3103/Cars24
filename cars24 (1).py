# -*- coding: utf-8 -*-
"""Cars24.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sid_ZiOOVAIsfx-d0vqDjNZ7X1ELxt78
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
from scipy import stats
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split, RandomizedSearchCV, KFold, GridSearchCV

from sklearn.linear_model import LinearRegression, Lasso
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, VotingRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.metrics import r2_score, mean_absolute_percentage_error
from xgboost import XGBRegressor
import pickle
import warnings
warnings.filterwarnings('ignore')

data = pd.read_excel('data (1).xlsx',sheet_name='data')
dictionary = pd.read_excel('data (1).xlsx',sheet_name='fields summary')

dictionary

df = data

df.head()

df.shape

df.describe()

df.info()

cx dxer

missing_values = df.isnull().sum().sort_values(ascending=False)
missing_values[missing_values > 0]

comment_columns = dictionary[dictionary['Description'] == 'current condition if not yes']['Columns'].tolist()

comment_columns

for col in comment_columns:
        df[col] = df[col].fillna('yes')

missing_values = df.isnull().sum().sort_values(ascending=False)
missing_values[missing_values > 0]

missing_cols = list(missing_values[missing_values > 0].index)
missing_cols

for col in missing_cols:
    unique_count = df[col].nunique()
    unique_values = df[col].unique()
    print(f"Column '{col}':")
    print(f"  Number of unique elements: {unique_count}")
    print(f"  Unique values: {unique_values}")
    print("-" * 20)

for col in missing_cols:
    count = df[col].count()
    total_count = len(df[col])
    print(f"Total values in {col}: Non-missing = {count}, Total (including NaN) = {total_count}")

comment_columns = dictionary[dictionary['Description'] == 'comments']['Columns'].tolist()

df = df.drop(columns=comment_columns, errors='ignore')

inspection_day = df['inspectionStartTime'].dt.day
inspection_month = df['inspectionStartTime'].dt.month
inspection_year = df['inspectionStartTime'].dt.year
inspection_hour = df['inspectionStartTime'].dt.hour

df['inspection_day'] = inspection_day
df['inspection_month'] = inspection_month
df['inspection_year'] = inspection_year
df['inspection_hour'] = inspection_hour

current_year = df['inspection_year']

df['Vehicle_age'] = current_year - df['year']

df['Vehicle_age'].fillna(0, inplace = True)
df['Vehicle_age'] = df['Vehicle_age'].astype(int)

df.shape

df

df.describe()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def value_counts_analysis(col_name):

    if col_name not in df.columns:
        print(f"Column '{col_name}' not found in the DataFrame.")
        return
    value_counts = df[col_name].value_counts(dropna=False)
    print(f"\nValue Counts for '{col_name}':\n{value_counts}")
    value_counts.plot(kind='bar', color='skyblue')
    plt.title(f"Frequency Distribution of '{col_name}'")
    plt.xlabel('Values')
    plt.ylabel('Count')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.show()


def missing_values_analysis(col_name):

    if col_name not in df.columns:
        print(f"Column '{col_name}' not found in the DataFrame.")
        return
    missing_count = df[col_name].isnull().sum()
    total = len(df)
    print(f"\nMissing Values in '{col_name}': {missing_count} out of {total} ({(missing_count / total) * 100:.2f}%)")


def unique_values_analysis(col_name):

    if col_name not in df.columns:
        print(f"Column '{col_name}' not found in the DataFrame.")
        return
    unique_values = df[col_name].unique()
    print(f"\nUnique Values in '{col_name}': {unique_values}")
    print(f"Total Unique Values: {len(unique_values)}")


def boxplot_analysis(col_name):

    if col_name not in df.columns:
        print(f"Column '{col_name}' not found in the DataFrame.")
        return
    if not pd.api.types.is_numeric_dtype(df[col_name]):
        print(f"Column '{col_name}' is not numeric. Boxplot is not applicable.")
        return
    sns.boxplot(x=df[col_name])
    plt.title(f"Boxplot of '{col_name}'")
    plt.show()


def histogram_analysis(col_name):

    if col_name not in df.columns:
        print(f"Column '{col_name}' not found in the DataFrame.")
        return
    if not pd.api.types.is_numeric_dtype(df[col_name]):
        print(f"Column '{col_name}' is not numeric. Histogram is not applicable.")
        return
    sns.histplot(df[col_name], kde=True, color='green', bins=10)
    plt.title(f"Histogram of '{col_name}'")
    plt.xlabel(col_name)
    plt.ylabel('Frequency')
    plt.show()


def barplot_analysis(col_name):

    if col_name not in df.columns:
        print(f"Column '{col_name}' not found in the DataFrame.")
        return
    if pd.api.types.is_numeric_dtype(df[col_name]):
        print(f"Column '{col_name}' is numeric. Bar plot is more suitable for categorical data.")
        return
    value_counts = df[col_name].value_counts()
    sns.barplot(x=value_counts.index, y=value_counts.values, palette='viridis')
    plt.title(f"Bar Plot of '{col_name}'")
    plt.xlabel(col_name)
    plt.ylabel('Count')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.show()

def univariate_analysis(col_name):
  value_counts_analysis(col_name)
  print()
  missing_values_analysis(col_name)
  print()
  unique_values_analysis(col_name)
  print()
  boxplot_analysis(col_name)
  print()
  histogram_analysis(col_name)
  print()
  barplot_analysis(col_name)

univariate_analysis("odometer_reading")

univariate_analysis("engineTransmission_battery_cc_value_0")

df

"""Bivariate Analysis"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

def categorical_independent_analysis_boxplot(dependent_var, categorical_independent_vars):
    for col_x in categorical_independent_vars:
        if col_x not in df.columns or dependent_var not in df.columns:
            print(f"One or both columns '{col_x}' or '{dependent_var}' not found in the DataFrame.")
            continue

        sns.boxplot(x=df[col_x], y=df[dependent_var])
        plt.title(f"Boxplot: {col_x} vs {dependent_var}")
        plt.xlabel(col_x)
        plt.ylabel(dependent_var)
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        plt.show()

dependent_var = "rating_engineTransmission"
categorical_independent_vars = [
    "engineTransmission_battery_value",
    "engineTransmission_engine_value",
    "engineTransmission_engineOil",
    "engineTransmission_gearShifting_value",
    "engineTransmission_clutch_value",
    "engineTransmission_engineSound_value",
    "engineTransmission_coolant_value",
    "engineTransmission_engineoilLevelDipstick_value",
    "engineTransmission_exhaustSmoke_value"]

numerical_independent_vars= ["odometer_reading","Vehicle_age"]

categorical_independent_analysis_boxplot(dependent_var, categorical_independent_vars)

def numerical_independent_analysis(dependent_var, numerical_independent_vars):
    for col_x in numerical_independent_vars:
        if col_x not in df.columns or dependent_var not in df.columns:
            print(f"One or both columns '{col_x}' or '{dependent_var}' not found in the DataFrame.")
            continue
        if not pd.api.types.is_numeric_dtype(df[col_x]):
            print(f"'{col_x}' should be numeric for analysis.")
            continue
        if not pd.api.types.is_numeric_dtype(df[dependent_var]):
            print(f"'{dependent_var}' should be numeric for analysis.")
            continue

        sns.scatterplot(x=df[col_x], y=df[dependent_var])
        plt.title(f"Scatter Plot: {col_x} vs {dependent_var}")
        plt.xlabel(col_x)
        plt.ylabel(dependent_var)
        plt.tight_layout()
        plt.show()

numerical_independent_analysis(dependent_var, numerical_independent_vars)

numeric_cols = df.select_dtypes(include=np.number).columns
numeric_cols

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

def correlation_analysis(df, numeric_cols):

    numeric_df = df[numeric_cols]

    correlation_matrix = numeric_df.corr()

    print("Correlation Analysis:")
    print(correlation_matrix)

def heatmap_correlation(df, numeric_cols):

    numeric_df = df[numeric_cols]

    corr_matrix = numeric_df.corr()

    plt.figure(figsize=(10, 8))
    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
    plt.title('Correlation Heatmap', fontsize=16)
    plt.tight_layout()
    plt.show()

def bivariate_analysis_categorical(df, dependent_variable, categorical_vars):
    for cat_var in categorical_vars:
        if cat_var in df.columns and dependent_variable in df.columns:
            sns.countplot(x=df[cat_var], hue=df[dependent_variable], palette="viridis")
            plt.title(f'Countplot: {dependent_variable} vs {cat_var}')
            plt.xlabel(cat_var)
            plt.ylabel('Count')
            plt.xticks(rotation=45, ha='right')
            plt.tight_layout()
            plt.show()

def bivariate_analysis_numerical(df, dependent_variable, numerical_vars):
    for num_var in numerical_vars:
        if num_var in df.columns and dependent_variable in df.columns:
            sns.scatterplot(x=df[num_var], y=df[dependent_variable])
            plt.title(f'Scatter Plot: {dependent_variable} vs {num_var}')
            plt.xlabel(num_var)
            plt.ylabel(dependent_variable)
            plt.tight_layout()
            plt.show()

bivariate_analysis_categorical(df, dependent_var, categorical_independent_vars)

bivariate_analysis_numerical(df, dependent_var, numerical_independent_vars)

heatmap_correlation(df, numeric_cols)

"""From above graph its clearly visible that odometer_reading and vehicle_age columns show negative correlation with the rating and the correlation values are also significant which is very intuitive as well"""

columns = ["Vehicle_age","odometer_reading"]
def outlier_analysis(df, columns):
    for col in columns:
        if col not in df.columns:
            print(f"Column '{col}' not found in the DataFrame.")
            continue

        if not pd.api.types.is_numeric_dtype(df[col]):
            print(f"Column '{col}' is not numeric, skipping outlier analysis.")
            continue

        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
        print(f"\nOutlier Analysis for '{col}':")
        print(f"  Q1: {Q1}")
        print(f"  Q3: {Q3}")
        print(f"  IQR: {IQR}")
        print(f"  Lower Bound: {lower_bound}")
        print(f"  Upper Bound: {upper_bound}")
        print(f"  Number of outliers: {len(outliers)}")

        plt.figure(figsize=(8, 6))
        sns.boxplot(x=df[col])
        plt.title(f"Boxplot of '{col}' with Outliers")
        plt.show()

outlier_analysis(df, ["Vehicle_age", "odometer_reading"])

def remove_outlier(df, col):

  q1 = np.quantile(df[col], .25)
  q3 = np.quantile(df[col], .75)

  IQR = q3 - q1

  upper_limit = q3 + 1.5 * IQR
  lower_limit = q1 - 1.5 * IQR

  return df[(df[col] < upper_limit) & (df[col] > lower_limit)]

remove_outlier(df, "odometer_reading")

remove_outlier(df, "Vehicle_age")

df.drop(['appointmentId', 'inspectionStartTime', 'inspection_year','inspection_day'], axis=1, inplace=True)

cols = list(df.columns)
cols.remove('rating_engineTransmission')
cols.append('rating_engineTransmission')
df = df[cols]

df.head()

train = df.iloc[:, :-1].reset_index(drop=True)
target = df.iloc[:, -1].reset_index(drop=True)

train_encoded = pd.get_dummies(train, drop_first=True)

mms = MinMaxScaler()
X = pd.DataFrame(mms.fit_transform(train_encoded), columns=train_encoded.columns)
y = target

train.shape

target.shape

train_encoded.head()

X.shape
X.head()

y.head()
y.shape

X_constant = sm.add_constant(X)
ols_model = sm.OLS(y, X_constant)
results = ols_model.fit()

print(results.summary())

from statsmodels.stats.diagnostic import linear_rainbow
import statsmodels.api as sm

X_with_constant = sm.add_constant(X)  # Add intercept to the model
model = sm.OLS(y, X_with_constant).fit()
rainbow_stat, rainbow_p_val = linear_rainbow(model)
print(f"Rainbow Test p-value: {rainbow_p_val}")
if rainbow_p_val < 0.05:
    print("The relationship is likely non-linear.")
else:
    print("The relationship is likely linear.")

from sklearn.decomposition import PCA

pca = PCA(n_components=0.90)
X_pca = pca.fit_transform(X)

print(f"Original number of features: {X.shape[1]}")
print(f"Reduced number of features: {X_pca.shape[1]}")

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# knn = KNeighborsRegressor()

# # Define the grid of hyperparameters to search
# param_grid = {
#     'n_neighbors': [3, 5, 7, 9, 11, 15, 20],  # Number of neighbors
#     'weights': ['uniform', 'distance'],       # Weight function
#     'metric': ['euclidean', 'manhattan']      # Distance metric
# }

# # Initialize GridSearchCV
# grid_search = GridSearchCV(estimator=knn, param_grid=param_grid,
#                            cv=5, scoring='r2', verbose=1, n_jobs=-1)

# # Fit the model to find the best hyperparameters
# grid_search.fit(X_train, y_train)

# # Get the best parameters and best estimator
# best_params = grid_search.best_params_
# best_knn = grid_search.best_estimator_

# print("Best Hyperparameters:", best_params)

# # Evaluate the tuned model on training and validation data
# train_preds = best_knn.predict(X_train)
# val_preds = best_knn.predict(X_val)

# knn_train_score = r2_score(y_train, train_preds)
# knn_val_score = r2_score(y_val, val_preds)

# print(f"Tuned KNN Train R2 Score: {knn_train_score}")
# print(f"Tuned KNN Validation R2 Score: {knn_val_score}")

knn = KNeighborsRegressor(n_neighbors=7, metric='manhattan', weights='distance')

knn.fit(X_train, y_train)

train_r2 = knn.score(X_train, y_train)
y_pred_test = knn.predict(X_test)
test_r2 = r2_score(y_test, y_pred_test)

print(f"Train R2: {train_r2}")
print(f"Test R2: {test_r2}")

from sklearn.svm import SVR

svr = SVR()
svr.fit(X_train, y_train)

# Print the R squared scores
svr_train_score = svr.score(X_train, y_train)
svr_test_score = svr.score(X_test, y_test)
print(svr_train_score, svr_test_score)

# dt = DecisionTreeRegressor(random_state=42)

# # Define the grid of hyperparameters to search
# param_grid = {
#     'max_depth': [3, 5, 10, 15, None],              # Maximum depth of the tree
#     'min_samples_split': [2, 5, 10, 20],            # Minimum samples to split a node
#     'min_samples_leaf': [1, 2, 5, 10],              # Minimum samples at a leaf node
#     'max_features': ['auto', 'sqrt', 'log2', None], # Number of features to consider
#     'criterion': ['squared_error', 'friedman_mse']  # Splitting criterion
# }

# # Initialize GridSearchCV
# grid_search = GridSearchCV(estimator=dt, param_grid=param_grid,
#                            cv=5, scoring='r2', verbose=1, n_jobs=-1)

# # Fit the model to find the best hyperparameters
# grid_search.fit(X_train, y_train)

# # Get the best parameters and best estimator
# best_params = grid_search.best_params_
# best_dt = grid_search.best_estimator_

# print("Best Hyperparameters:", best_params)

# # Evaluate the tuned model on training and validation data
# train_preds = best_dt.predict(X_train)
# val_preds = best_dt.predict(X_val)

# dt_train_score = r2_score(y_train, train_preds)
# dt_val_score = r2_score(y_val, val_preds)

# print(f"Tuned Decision Tree Train R2 Score: {dt_train_score}")
# print(f"Tuned Decision Tree Validation R2 Score: {dt_val_score}")

from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import r2_score

dt = DecisionTreeRegressor(criterion='squared_error',
                           max_depth=10,
                           max_features=None,
                           min_samples_leaf=10,
                           min_samples_split=2)

dt.fit(X_train, y_train)

train_r2 = dt.score(X_train, y_train)

y_pred_test = dt.predict(X_test)
test_r2 = r2_score(y_test, y_pred_test)

print(f"Train R2: {train_r2}")
print(f"Test R2: {test_r2}")

# rf = RandomForestRegressor(random_state=42)

# # Define the grid of hyperparameters
# param_grid = {
#     'n_estimators': [50, 100, 200],          # Number of trees
#     'max_depth': [None, 10, 20, 30],         # Maximum depth of trees
#     'min_samples_split': [2, 5, 10],         # Minimum samples to split a node
#     'min_samples_leaf': [1, 2, 4],           # Minimum samples at a leaf node
#     'max_features': ['auto', 'sqrt', 'log2'],# Number of features to consider at a split
#     'bootstrap': [True, False]               # Whether bootstrap samples are used
# }

# grid_search = GridSearchCV(estimator=rf, param_grid=param_grid,
#                            cv=5, scoring='r2', verbose=2, n_jobs=-1)

# grid_search.fit(X_train, y_train)

# best_params = grid_search.best_params_
# best_rf = grid_search.best_estimator_

# print("Best Hyperparameters:", best_params)

# rf_train_preds = best_rf.predict(X_train)
# rf_val_preds = best_rf.predict(X_val)

# rf_train_score = r2_score(y_train, rf_train_preds)
# rf_val_score = r2_score(y_val, rf_val_preds)

# print(f"Tuned Random Forest Train R2 Score: {rf_train_score}")
# print(f"Tuned Random Forest Validation R2 Score: {rf_val_score}")

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score

rf = RandomForestRegressor(bootstrap=False,
                           max_depth=30,
                           max_features='sqrt',
                           min_samples_leaf=1,
                           min_samples_split=2,
                           n_estimators=200)

rf.fit(X_train, y_train)

train_r2 = rf.score(X_train, y_train)

y_pred_test = rf.predict(X_test)

print(f"Train R2: {train_r2}")
print(f"Test R2: {test_r2}")

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score

rf = RandomForestRegressor(bootstrap=False,
                           max_depth=20,
                           max_features='sqrt',
                           min_samples_leaf=1,
                           min_samples_split=2,
                           n_estimators=200)

rf.fit(X_train, y_train)

train_r2 = rf.score(X_train, y_train)

y_pred_test = rf.predict(X_test)

print(f"Train R2: {train_r2}")
print(f"Test R2: {test_r2}")

xgb = XGBRegressor()
xgb.fit(X_train, y_train)

xgb_train_score = xgb.score(X_train, y_train)
xgb_test_score = xgb.score(X_test, y_test)

print(f"XGB Train R2 Score: {xgb_train_score}")
print(f"XGB Test R2 Score: {xgb_test_score}")

xgb = XGBRegressor(max_depth=4)  # Default is 6
xgb.fit(X_train, y_train)

xgb_train_score = xgb.score(X_train, y_train)
xgb_test_score = xgb.score(X_test, y_test)

print(f"XGB Train R2 Score: {xgb_train_score}")
print(f"XGB Test R2 Score: {xgb_test_score}")



import pickle

with open('xgb_model.pkl', 'wb') as file:
    pickle.dump(xgb, file)

xgb = XGBRegressor()
xgb.fit(X_train, y_train)

feature_importances = xgb.feature_importances_

importance_df = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': feature_importances
})

importance_df = importance_df.sort_values(by='Importance', ascending=False)

print("Top Features by Importance:")
print(importance_df.head())

"""OUTLIER DETECTION

"""

y_pred = xgb.predict(X_test)

comparison_df = pd.DataFrame({
    'Actual': y_test,
    'Predicted': y_pred,
    'Difference': abs(y_test - y_pred)
})

comparison_df = comparison_df.sort_values(by='Difference', ascending=False)
print("Top Cases with Large Prediction Errors:")
print(comparison_df.head())

threshold = 1.5 * comparison_df['Difference'].std()

outliers_df = comparison_df[comparison_df['Difference'] > threshold]
print(f"Number of Outliers: {len(outliers_df)}")
print(outliers_df)

import pickle

with open('xgb_model.pkl', 'wb') as file:
    pickle.dump(xgb, file)

param_grid = {
    'C': [0.1, 1, 10, 100],         # Regularization parameter
    'epsilon': [0.01, 0.1, 0.2],    # Epsilon in the epsilon-SVR
    'kernel': ['linear', 'rbf', 'poly']  # Kernel types: linear, rbf (default), or poly
}
from sklearn.svm import SVR

svr = SVR()

grid_search = GridSearchCV(svr, param_grid, cv=5, scoring='r2', n_jobs=-1)  # 5-fold CV
grid_search.fit(X_train, y_train)

# Get the best model
best_svr = grid_search.best_estimator_

# Step 3: Evaluate the model
svr_train_score = best_svr.score(X_train, y_train)
svr_test_score = best_svr.score(X_test, y_test)

print(f"Tuned SVR Training R2 Score: {svr_train_score}")
print(f"Tuned SVR Test R2 Score: {svr_test_score}")

# Optionally, check the best parameters from the grid search
print(f"Best hyperparameters from GridSearch: {grid_search.best_params_}")

# 5. XGboost
xgb = XGBRegressor(random_state=42)

# Define the grid of hyperparameters
param_grid = {
    'n_estimators': [100, 200, 300],                # Number of trees
    'learning_rate': [0.01, 0.1, 0.2],              # Step size
    'max_depth': [3, 5, 7],                          # Maximum depth of trees
    'min_child_weight': [1, 3, 5],                   # Minimum sum of instance weight needed in a child
    'subsample': [0.7, 0.8, 1.0],                    # Fraction of samples to train each tree
    'colsample_bytree': [0.7, 0.8, 1.0],            # Fraction of features to train each tree
    'gamma': [0, 0.1, 0.2],                         # Minimum loss reduction required to make a partition
    'reg_alpha': [0, 0.01, 0.1],                    # L1 regularization term
    'reg_lambda': [1, 1.5, 2]                       # L2 regularization term
}

grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid,
                           cv=5, scoring='r2', verbose=1, n_jobs=-1)

grid_search.fit(X_train, y_train)

best_params = grid_search.best_params_
best_xgb = grid_search.best_estimator_

print("Best Hyperparameters:", best_params)

xgb_train_preds = best_xgb.predict(X_train)
xgb_test_preds = best_xgb.predict(X_test)

xgb_train_score = r2_score(y_train, xgb_train_preds)
xgb_test_score = r2_score(y_test, xgb_test_preds)

print(f"Tuned XGB Train R2 Score: {xgb_train_score}")
print(f"Tuned XGB Test R2 Score: {xgb_test_score}")

